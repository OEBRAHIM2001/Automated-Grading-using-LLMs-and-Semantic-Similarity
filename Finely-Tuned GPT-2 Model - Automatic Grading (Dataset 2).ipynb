{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finely-Tuned GPT-2 Model - Automatic Grading (Dataset 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omar Ebrahim - 202000443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omar\\anaconda3\\envs\\CS4120\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr \n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importing the dataset and defining the GPT2Tokenizer used for tokenization and the original GPTModel used for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Omar/OneDrive/Desktop/my_dataset.csv\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating a CustomDataset which replaces the sample and student answers with their tokenized versions, and adds from the original dataset the original question and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question = str(self.data.loc[index, 'Question'])\n",
    "        correct_code = str(self.data.loc[index, 'Correct_Code'])\n",
    "        code_with_error = str(self.data.loc[index, 'Code_with_Error'])\n",
    "        total_marks = self.data.loc[index, 'Total_Marks']\n",
    "        \n",
    "        inputs = self.tokenizer(correct_code,\n",
    "                                max_length=self.max_length, \n",
    "                                padding='max_length', \n",
    "                                truncation=True, \n",
    "                                return_tensors='pt')\n",
    "        \n",
    "        inputs2 = self.tokenizer(code_with_error,\n",
    "                                 max_length=self.max_length, \n",
    "                                 padding='max_length', \n",
    "                                 truncation=True, \n",
    "                                 return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'input_ids2': inputs2['input_ids'].flatten(),\n",
    "            'total_marks': torch.tensor(total_marks, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "dataset = CustomDataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creating the LoRA configuration with their default values, and then adding it to the BERTModel for the purpose of reducing trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,  # Task type for which the model will undergo fine-tuning\n",
    "    r=1,  # Dimensions of A and B\n",
    "    lora_alpha=1,  # Scaling factor determining the relative significance of weights in A and B\n",
    "    lora_dropout=0.1  # Dropout probability for LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omar\\anaconda3\\envs\\CS4120\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): lora.Linear(\n",
       "              (base_layer): Conv1D()\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=768, out_features=1, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=1, out_features=2304, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. End of Sequence condition added for the tokenized results such that they are able to be used for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generating the word embeddings for sample answers and the student answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2embeddings1 = []\n",
    "GPT2embeddings2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in DataLoader(dataset, batch_size=1):\n",
    "        input_ids = example['input_ids']\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        GPT2embeddings1.append(embeddings.squeeze().detach().numpy())\n",
    "\n",
    "    for example in DataLoader(dataset, batch_size=1):\n",
    "        input_ids2 = example['input_ids2']\n",
    "        outputs = model(input_ids=input_ids2)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        GPT2embeddings2.append(embeddings.squeeze().detach().numpy())\n",
    "\n",
    "GPT2embeddings1 = np.array(GPT2embeddings1)\n",
    "GPT2embeddings2 = np.array(GPT2embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluation:\n",
    "   1. Generating the new scores by doing cosine similarity between embedded student answers and embedded model answers (rescaled between 1-10)\n",
    "   2. After getting both scores, we compare them using using MAE and Pearson correlation\n",
    "   3. MAPE was used additionally to see the % difference between both grades per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Score | Predicted Score | MAPE (%)\n",
      "--------------------------------------------\n",
      "10.00           | 5.06            | 49.42\n",
      "8.00           | 5.05            | 36.84\n",
      "8.00           | 5.07            | 36.69\n",
      "8.00           | 5.06            | 36.69\n",
      "8.00           | 5.07            | 36.56\n",
      "10.00           | 5.05            | 49.47\n",
      "6.00           | 5.04            | 15.93\n",
      "10.00           | 5.05            | 49.48\n",
      "10.00           | 5.06            | 49.43\n",
      "10.00           | 5.06            | 49.40\n",
      "8.00           | 5.06            | 36.74\n",
      "8.00           | 5.07            | 36.67\n",
      "8.00           | 5.07            | 36.63\n",
      "10.00           | 5.06            | 49.40\n",
      "0.00           | 5.06            | inf\n",
      "10.00           | 5.06            | 49.37\n",
      "7.00           | 5.06            | 27.70\n",
      "8.00           | 5.07            | 36.64\n",
      "0.00           | 5.07            | inf\n",
      "0.00           | 5.05            | inf\n",
      "8.00           | 5.06            | 36.72\n",
      "10.00           | 5.07            | 49.27\n",
      "6.00           | 5.03            | 16.13\n",
      "6.00           | 5.10            | 15.07\n",
      "10.00           | 5.09            | 49.07\n",
      "10.00           | 5.09            | 49.12\n",
      "0.00           | 5.07            | inf\n",
      "0.00           | 5.06            | inf\n",
      "6.00           | 5.03            | 16.12\n",
      "8.00           | 5.08            | 36.51\n",
      "0.00           | 5.07            | inf\n",
      "0.00           | 5.05            | inf\n",
      "10.00           | 5.06            | 49.42\n",
      "0.00           | 5.07            | inf\n",
      "0.00           | 5.05            | inf\n",
      "8.00           | 5.06            | 36.79\n",
      "0.00           | 5.06            | inf\n",
      "10.00           | 5.06            | 49.35\n",
      "10.00           | 5.06            | 49.42\n",
      "10.00           | 5.05            | 49.48\n",
      "0.00           | 5.03            | inf\n",
      "10.00           | 5.02            | 49.82\n",
      "10.00           | 5.02            | 49.83\n",
      "10.00           | 5.02            | 49.76\n",
      "7.00           | 5.03            | 28.10\n",
      "8.00           | 5.04            | 37.03\n",
      "0.00           | 5.01            | inf\n",
      "10.00           | 5.03            | 49.67\n",
      "8.00           | 5.02            | 37.23\n",
      "10.00           | 5.03            | 49.69\n",
      "0.00           | 5.00            | inf\n",
      "10.00           | 5.01            | 49.88\n",
      "0.00           | 5.08            | inf\n",
      "10.00           | 5.01            | 49.91\n",
      "0.00           | 5.03            | inf\n",
      "0.00           | 5.02            | inf\n",
      "0.00           | 5.02            | inf\n",
      "10.00           | 5.03            | 49.69\n",
      "0.00           | 5.02            | inf\n",
      "0.00           | 5.03            | inf\n",
      "0.00           | 5.02            | inf\n",
      "5.00           | 5.03            | 0.61\n",
      "0.00           | 5.01            | inf\n",
      "0.00           | 5.04            | inf\n",
      "0.00           | 5.02            | inf\n",
      "0.00           | 5.03            | inf\n",
      "0.00           | 5.02            | inf\n",
      "0.00           | 5.02            | inf\n",
      "5.00           | 5.03            | 0.53\n",
      "6.00           | 5.01            | 16.44\n",
      "0.00           | 5.04            | inf\n",
      "0.00           | 5.02            | inf\n",
      "\n",
      "Mean Absolute Error: 4.12\n",
      "Pearson Correlation: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
      "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_61352\\1095776795.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mape = np.abs((predicted_score - original_score) / original_score) * 100\n"
     ]
    }
   ],
   "source": [
    "MAE = []\n",
    "MAPE = []\n",
    "Pearson = []\n",
    "predicted_scores = []\n",
    "\n",
    "GPT2embeddings1 = np.load('GPT2embeddings1.npy', allow_pickle=True)\n",
    "GPT2embeddings2 = np.load('BMwordEmbeddings2.npy', allow_pickle=True)\n",
    "\n",
    "print(\"Original Score | Predicted Score | MAPE (%)\")\n",
    "print(\"--------------------------------------------\")\n",
    "for i, row in df.iterrows():\n",
    "    GPT2embeddings1_i = GPT2embeddings1[i].reshape(1, -1)\n",
    "    GPT2embeddings2_i = GPT2embeddings2[i].reshape(1, -1)\n",
    "\n",
    "    similarity = cosine_similarity(GPT2embeddings1_i, GPT2embeddings2_i)[0][0]\n",
    "    scaled_similarity = (similarity + 1) * 5\n",
    "    predicted_score = max(min(scaled_similarity, 10), 0)\n",
    "    original_score = row['Total_Marks']\n",
    "\n",
    "    mape = np.abs((predicted_score - original_score) / original_score) * 100\n",
    "\n",
    "    print(f\"{original_score:.2f}           | {predicted_score:.2f}            | {mape:.2f}\")\n",
    "\n",
    "    MAE.append(np.abs(predicted_score - original_score))\n",
    "    MAPE.append(mape)\n",
    "    Pearson.append((original_score, predicted_score))\n",
    "    predicted_scores.append(predicted_score) \n",
    "\n",
    "MAE = np.mean(MAE)\n",
    "MAPE = np.mean(MAPE)\n",
    "pearson_corr = np.array(Pearson)\n",
    "corr_coefficient, _ = pearsonr(pearson_corr[:,0], pearson_corr[:,1])\n",
    "\n",
    "print(f\"\\nMean Absolute Error: {MAE:.2f}\")\n",
    "print(f\"Pearson Correlation: {corr_coefficient:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
